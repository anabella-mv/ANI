\documentclass[a4paper,12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\title{Teorico Final 2025}
\author{Mai Valdez}

\begin{document}
\maketitle

\section{Introducción}
Los teoremas y demostraciones solicitados para el final de Analisis Numerico I - 2025

\section{Teórico}
\subsection{Teorema: Metodo de Bisección}
\subsubsection{Enunciado:}
Sea $f: [a,b] \to \mathbb{R}$ una funcion continua tal que $f(a)f(b)<0$. Si $[a_0,b_0]$, $\dots$, $[a_n,b_n]$, $\dots$ denotan los sucesivos intervalos en el método de bisección, 
entonces existen los límites: $\lim_{n \to \infty} a_n$ y $\lim_{n \to \infty} b_n$, son iguales y representan una raiz de $f$. 
Si $c_n=\frac{1}{2}(a_n+b_n)$ y $r= \lim_{n \to \infty} c_n$ entonces $\lvert r-c_n \rvert \leq \frac{1}{2^{n+1}}(b_0-a_0)$

\subsubsection{Demostración:}
Si son los intervalos generados por el metodo de bisección, entonces:

\begin{gather*}
a_0\leq a_1\leq \dots \leq b_0 \\
b_0\geq b_1\geq \dots \geq a_0
\end{gather*}

Luego $\{a_n\}$ es una sucesión creciente y acotada superiormente, entonces $\{a_n\}$ es convergente y análogamente $\{b_n\}$ es decreciente y acotada inferiormente, por lo tantp también es convergente. Además,

\[
    b_n-a_n= \frac{1}{2}(b_{n-1}-a_{n-1}), \quad n\geq 1
\]

Aplicando repetidamente se obtiene:
\[ 
 b_n-a_n= \frac{1}{2^n}(b_0-a_0)
\]
Ahora trabajo con límites y resulta

\[
 \lim_{n \to \infty}b_n - \lim_{n \to \infty}a_n= \lim_{n \to \infty}(b_n-a_n)=\lim_{n \to \infty}\frac{1}{2^n}(b_0-a_0)=0
\]

Sea $r=\lim_{n \to \infty}a_n=\lim_{n \to \infty}b_n$, por lo tanto, tomando límite en la desigualdad $f(a_n)f(b_n)<0$, se obtiene $f(r^2)\leq 0$. De allí que $f(r^2)=0$ y en consecuencia $f(r)=0$, es decir, r es la raíz de $f$. Finalmente, sea $c_n=\frac{1}{2}(a_n+b_n)$, luego 
\[
\lvert r-c_n \rvert \leq \frac{1}{2}\lvert b_n-a_n\rvert \leq \frac{1}{2^{n+1}}(b_0-a_0)
\]

\subsection{Teorema: Método de Newton}
\subsubsection{Enunciado:}
Si $f''$ es continua en un entorno de una raíz r de $f$ y $f'(r)\neq 0$ entonces $\exists \delta >0$ tal que si el punto incicial $x_0$ satisface $\lvert r-x_0 \rvert \leq \delta$, luego, todos los puntos generados por el algoritmno de Newton($x_n=x_{n-1}-\frac{f(x_{n-1})}{f'(x_{n-1})}$) en la sucesion $\{x_n\}$ satisfacen que $\lvert r-x_n \rvert \leq\delta \quad \forall n$, la sucesion $\{x_n\}$ converge a $r$ y la convergencia es cuadrática, es decir que existen una constante $c$ y un natural $N$ tal que $c=c(\delta)$ y $\lvert r-x_{n+1}\rvert \leq c\lvert r-x_n\rvert ^2$ para $n \geq N$

\subsection{Teorema: Método de iteración de punto fijo}
\subsubsection{Enunciado:}
Sea $g \in C[a,b]$ tal que $g(x)\in [a,b], \forall x \in [a,b]$. Supongamos existen $g'(x), \forall x \in (a,b)$ y una constante positiva $k$, $(0<k<1)$ tal que $\lvert g'(x)\rvert \leq k, \forall x \in (a,b)$, entonces para cualquier $p_0\in [a,b]$ la sucesion definida por $p_n=g(p_{n-1})$ para $n \geq 1$ converge al único punto fijo en $(a,b)$ 

\subsubsection{Demostración:}
Sabemos existe un unico punto fijo $p \in [a,b]$. Cómo $g(x) \in [a,b], \forall x \in [a,b]$ la sucesión de aproximaciones $\{p_n\}$ está bien definida $\forall n$, es decir, $p_n \in [a,b] \forall n$. Para probar la convergencia se usa el teorema del valor medio en lo siguiente:
\[
\lvert p_n-p\rvert = \lvert g(p_{n-1})-g(p)\rvert = \lvert g'(\xi_n ) \rvert \lvert p_{n-1}-p \rvert \leq k \lvert p_{n-1}-p\rvert
\]
Luego por recurrencia, se tiene que:

\[
\lvert p_n-p\rvert \leq k \lvert p_{n-1}-p\rvert \leq k^2 \lvert p_{n-2}-p\rvert \leq \dots \leq k^n \lvert p_0-p\rvert
\]

Como $0<k<1$ entonces $\lim_{n \to \infty}k^n=0$, luego 

\[
\lim_{n \to \infty}\lvert p_n-p\rvert \leq \lim_{n \to \infty}k^n \lvert p_0-p\rvert \\
=  \lvert p_0-p\rvert\lim_{n \to \infty}k^n =0
\]
Por lo tanto, la sucesión $\{p_n\}$ converge al punto fijo p.

\subsection{Corolario: Cotas del error en el método de pto. fijo}
\subsubsection{Enunciado:}
Si $g$ es una función que satisface las condiciones del teorema anterior se tienen las siguientes cotas del error:
\begin{gather*}
    \lvert p_n-p\rvert \leq k^n \max\{p_0-a,b-p_0\} \\
    \lvert p_n-p\rvert \leq \frac{k^n}{1-k}\lvert p_1-p_0\rvert
\end{gather*}
\subsubsection{Demostración:}
Para la primera desigualdad: Se demostró en el teorema anterior que $\lvert p_n-p\rvert \leq k^n \lvert p_0-p \rvert$ y además como $p \in [a,b]$ y $p_0 \in [a,b]$ entonces $\lvert p_0-p\rvert \leq \max\{p_0-a,b-p_0\}$, por lo tanto:
\[
\lvert p_n-p\rvert \leq k^n \lvert p_0-p\rvert \leq k^n \max\{p_0-a,b-p_0\}
\]
Para la segunda desigualdad: Para $n\geq 1$, el procedimiento de iteración muestra que:
\[
    \lvert p_{n+1}-p_n \rvert = \lvert g(p_n)-g(p_{n-1})\rvert \leq k \lvert p_n - p_{n-1} \rvert \leq \dots \leq k^n \lvert p_1-p_0\rvert
\]
$m>n\geq 1$
\begin{gather*}
    \lvert p_m-p_n \rvert = \lvert p_m-p_{m-1} + p_{m-1}- \dots + p_{n+1}-p_n \rvert \\
    \leq \lvert p_m-p_{m-1} \rvert + \lvert p_{m-1}-p_{m-2} \rvert + \dots + \lvert p_{n+1}-p_n \rvert \\
    \leq k^{m-1} \lvert p_1-p_0\rvert + k^{m-2} \lvert p_1-p_0\rvert + \dots + k^n \lvert p_1-p_0\rvert \\ 
    = k^n \lvert p_1-p_0\rvert (1+k+k^2+\dots +k^{m-n-1}) 
\end{gather*}
Ahora cuando $\lim_{m \to \infty} p_m=p$, por lo que
\[
\lvert p-p_n\rvert=\lim_{m \to \infty}\lvert p_m-p_n\rvert \leq \lim_{m \to \infty} k^n \lvert p_1-p_0\rvert \sum_{i = 0}^{m-n-1} k^i\leq k^n \lvert p_1-p_0\rvert \sum_{i=0}^{\infty} k^i
\]

Pero $\sum_{i=0}^{\infty}k^i$ es una serie geometrica con radio $k$ y $0<k<1$. Esta sucesión converge a $\frac{1}{1-k}$, lo que nos da la segunda cota:
\[
\lvert p_n-p\rvert \leq \frac{k^n}{1-k}\lvert p_1-p_0\rvert
\]

\subsection{Teorema: Unicidad del polinomio interpolante}
\subsubsection{Enunciado:}

Dados $x_0, x_1,\dots ,x_n$ números reales distintos con valores asociados $y_0, y_1, \dots ,y_n$ entonces existe un único polinomio $p_n$ de grado menor o igual a $n$ tal que $p_n(x_i)=y_i$ para $i=0,1,\dots ,n$.
\subsubsection{Demostración:}
Supongamos que existen dos polinomios interpolantes $p_n$ y $q_n$ de grado menor o igual a $n$ , esto es, $p_n(x_i)=y_i$ y $q_n(x_i)=y_i$ para $i=0,1,\dots ,n$.\\
Sea $h=p_n-q_n$. Claramente $h$ es un polinomio de grado $\leq n$. Además $h(x_i)=0$ para $i=0,1,\dots ,n$. Por lo tanto, $h$  es un polinomio de grado $\leq n$ y tiene $n+1$ raíces distintas reales. Luego, por el teorema fundamental del álgebra, $h(x)=0$ para todo $x$ y por lo tanto, $p_n(x)=q_n(x)$

\subsection{Formas de Newton del polinomio interpolante}
\subsubsection{Enunciado:}
La forma de Newton compacta del polinomio interpolante resulta en:
\[
p_k(x)=\sum_{i=0}^{k}c_i\prod_{j=0}^{i-1}(x-x_j)
\]
Aquí se adopra la convención de que $\prod_{j=0}^{m} (x-x_j)=1$ si $m<0$

\subsection{Formas de Lagrange del polinomio interpolante}
\subsubsection{Enunciado:}
La forma de Lagrange del polinomio interpolante resulta en:
\[
p_n(x)=\sum_{i=0}^{n}y_il_i(x)
\]
Con $l_i(x)=\prod_{j=0, j\neq i}^{n}\frac{x-x_j}{x_i-x_j}$, es decir, $l_i(x)$ es $1$ si $i=j$ y $0$ en caso contrario.
\subsection{Teorema: Fórmula del error del polinomio interpolante}
\subsubsection{Enunciado:}
Sea $f$ una funcion en $C^{n+1}[a,b]$ y $p$ un polinomio de grado menor o igual a $n$ que interpola a $f$ en $n+1$ puntos distintos $x_0, x_1, \dots , x_n$ en $[a,b]$. Entonces para cada $x\in [a,b]$ existe un $\xi = \xi _x \in (a,b)$ tal que 
\[
f(x)-p(x)=\frac{1}{(n+1)!}f^{(n+1)}(\xi)\prod_{i=0}^{n}(x-x_i)
\]

\subsection{Observación: Fórmula del error de Spline lineal para puntos equidistribuidos}

\subsubsection{Enunciado:}

Supongamos que $f$ es 2 veces derivable en $[a,b]$ y que $x_k=a+kh$ para $k=0,1,\dots ,n$ con $h=\frac{b-a}{n}$. \\
Si $S$ es el spline lineal, en cada intervalo $[x_k,x_{k+1}]$ se tiene un polinomio de grado $\leq 1$. Entonces el error de interpolación para cada $x\in [a,b]$ es:
\[
\lvert e(x) \rvert \leq \frac{M}{8}h^2
\]
donde $\lvert f''(x)\rvert\leq M$ para todo $x\in [a,b]=[x_0,x_n]$.

\subsubsection{Demostración:}

Sea $S$ el spline lineal que interpola a $f$ en los puntos $x_k$, $k=0,1,\dots ,n$. Entonces, en cada intervalo $[x_k,x_{k+1}]$, el spline lineal es un polinomio de grado $\leq 1$. Por lo tanto, el error de interpolación en cada intervalo se puede expresar como:
\[
\lvert f(x)-S(x) \rvert = \lvert \frac{(x-x_i)(x_{i+1}-1)}{2}f''(\xi) \rvert \leq \frac{(x-x_i)(x_{i+1}-x)}{2} \max \limits_{x \in [a,b]} \lvert f''(x) \rvert
\]

Como $(x-x_i)(x_{i+1}-x)$ es un producto de dos números positivos en $(x_i,x_{i+1})$, y alcanza su máximo en el punto medio del intervalo, en $x=\frac{x_i+x_{i+1}}{2}$ se tiene:
\[ 
(x-x_i)(x_{i+1}-x) \leq (\frac{h}{2})^2 = \frac{h^2}{4}
\]
Por lo tanto, el error de interpolación en cada intervalo es:
\[
\lvert f(x)-S(x) \rvert \leq \frac{1}{2} \frac{h^2}{4} \max \limits_{x \in [a,b]} \lvert f''(x) \rvert = \frac{h^2}{8}\max \limits_{x \in [a,b]} \lvert f''(x) \rvert 
\]

\subsection{Teorema: Valor medio de integración}
\subsubsection{Enunciado:}

Supongamos que $f$ es continua en $[a,b]$. que $g$ es una función integrable en $[a,b]$ y que $g$ no cambia de signo en $[a,b]$. Entonces existe $c \in (a,b)$ tal que:
\[
\int_a^b f(x)g(x)dx = f(c)\int_a^b g(x)dx
\]
En particular, si $g(x)\equiv 1$, entonces $\int_a^b f(x)dx=f(c)(b-a)$, esto es, $f(c)=\frac{1}{b-a}\int_a^b f(x)dx$. Como $g(x)=(x-x_0)(x-x_1)=(x-a)(x-b)$ no cambia de signo en $[a,b]$ y aplicando teorema se sabe que existe $\xi$ independiente de $x$ tal que:

\subsubsection{Demostración:}
Dado que $f\in C[a,b]$ por el teorema del valor extremo, existen mínimo ymáximo de $f$ en el intervalo. Sea $m:= \min \limits_{x \in [a,b]} f(x)$ y $M:= \max \limits_{x \in [a,b]} f(x)$, Cómo $g$ no cambia de signo en $[a,b]$, podemos multiplicar por $g$ y obtenemos:
\[
m\cdot g(x) \leq f(x)g(x) \leq M\cdot g(x) \quad \forall x \in [a,b]
\]

Integrando en $[a,b]$ resulta: $m\int_a^b g(x)dx \leq \int_a^b f(x)g(x)dx \leq M\int_a^b g(x)dx$. Y llamamos $G=\int_a^b g(x)dx$ e $I=\int_a^b f(x)g(x)dx$.
Si $G>0$, entonces como $g\geq 0$ debe ser $g \equiv 0$, por lo tanto:
\[
\int_a^b f(x)g(x)dx = 0 = f(c)\int_a^b g(x)dx=f(c)\cdot 0 \quad.
\]
Se cumple trivialmente para todo $c \in [a,b]$. Supongamos ahora $G\neq 0$. Entonces, $m\leq \frac{I}{G}\leq M$. Como $\frac{I}{G} \in [m,M]$ y $f$ es continua, por el teorema del calor intermedio, existe $c \in [a,b]$ tal que:
\[
f(c)=\frac{I}{G}=\frac{\int_a^b f(x)g(x)dx}{\int_a^b g(x)dx} \Rightarrow \int_a^b f(x)g(x)dx = f(c)\int_a^b g(x)dx 
\]
Además como $f$ es continua y $g\neq 0$ el valor de $c$ se puede elegir del intervalo $(a,b)$.
Por lo tanto se cumple:\\

$\qquad\int_a^b f(x)g(x)dx = f(c)\int_a^b g(x)dx \quad$ para algun $c \in (a,b)$

\subsection{Fórmula del error para la regla del trapecio simple}
\subsubsection{Enunciado:}
La regla del trapecio simple para integración numérica en el intervalo $[a,b]$ está dada por:
\[
\int_a^b f(x)dx \approx \frac{b-a}{2}(f(a)+f(b))=\frac{h}{2}(f(a)+f(b))
\]
 Y su correspondiente error de aproximación es:
\[
E_T=-\frac{(b-a)^3}{12}f''(\xi)=-\frac{h^3}{12}f''(\xi)
\]
\subsubsection{Demostración:}
Como $g(x)=(x-x_0)(x-x_1)=(x-a)(x-b)$ no cambia de signo en $[a,b]$ y aplicando el teorema anterior se sabe que existe $\xi$ independiente de $x$ tal que:

\begin{align*}
    \int_a^b f''(\xi_x)(x-a)(x-b)dx &= f''(\xi)\int_a^b (x-a)(x-b)dx \\
    &= f''(\xi)\int_a^b (x^2-(a+b)x+ab)dx \\
    &= f''(\xi)\left[\frac{x^3}{3}-\frac{(a+b)x^2}{2}+abx\right]_a^b \\
\end{align*}

Luego, evaluando en $a$ y $b$:

\begin{align*}
    \left[\frac{x^3}{3}-\frac{(a+b)x^2}{2}+abx\right]_a^b &= \frac{b^3}{3} - \frac{ab^2}{2} - \frac{b^3}{2} + ab^2 - \frac{a^3}{3} + \frac{a^3}{2} + \frac{a^2 b}{2}-a^2b\\
    &= \frac{1}{6}(2b^3 - 3ab^2 - 3b^3 + 6ab^2 - 2a^3 + 3a^3 + 3a^2b - 6a^2b) \\
    &= \frac{1}{6}(-b^3 + 3ab^2 + a^3 - 3a^2b) \\
    &= \frac{(a-b)^3}{6}= -\frac{(b-a)^3}{6}= -\frac{h^3}{6}
\end{align*}

Por lo tanto si el correspondiente error es $e_1(x)=\frac{f''(\xi_x)}{2!}(x-x_0)(x-x_1)$, y los cálculos anteriores:

\begin{align*}
    E_T=E_1(x) &= \int_a^b e_1(x)dx =\frac{1}{2!} \int_a^b f''(\xi_x)(x-a)(x-b)dx \\
    &= \frac{1}{2!}f''(\xi)\left( -\frac{h^3}{6} \right)\\
    &= -\frac{(b-a)^3}{12}f''(\xi) = -\frac{h^3}{12}f''(\xi)
\end{align*}

Para algun $\xi \in (a,b)$.

\subsection{Fórmula del error para la regla del trapecio compuesta para nodos equidistribuidos}
\subsubsection{Enunciado:}
Sean $f \in C^2[a,b]$, $n$ un entero positivo, $h=\frac{b-a}{n}$ y $x_j=a+jh$ para $j=0,1,\dots ,n$. Entonces existe $\mu \in (a,b)$ tal que la regla compuesta del trapecio para $n$ subintervalos está dada por:
\[
\int_a^b f(x)dx = \frac{h}{2}\left(f(a)+f(b)+2\sum_{j=1}^{n-1} f(x_j) \right) - \frac{(b-a)}{12}h^2f''(\mu)
\]
\subsubsection{Demostración:}

Se comienza particionando el intervalo $[a,b]$ en $n$ subintervalos y luego se aplica la regla simple del trapecio en cada subintervalo:
\begin{align*}
    \int_a^b f(x)dx &= \sum_{j=1}^{n} \int_{x_{j-1}}^{x_j} f(x)dx \\
    &= \sum_{j=1}^{n} \left(\frac{h}{2}(f(x_{j-1})+f(x_j)) - \frac{h^3}{12}f''(\xi_j)\right) 
\end{align*}
Para algun $\xi_j \in (x_{j-1},x_j)$, con $j=1,2,\dots ,n$ y $f \in C^2[a,b]$. Si observamos los valores $f(x_j)$ para $j=1,2,\dots ,n-1$, aparecen dos veces en la última expresion, por lo tanto puedo reescribirlo:
\[
\int_a^b f(x)dx = \frac{h}{2}\left(f(x_0)+f(x_n)+2\sum_{j=1}^{n-1} f(x_j)\right) - \frac{h^3}{12}\sum_{j=1}^{n}f''(\xi_j)
\]
Para algun $\xi_j \in (x_{j-1},x_j)$, con $j=1,2,\dots ,n$. Ahora, consideramos el término del error:
\[
E(f) = -\frac{h^3}{12}\sum_{j=1}^{n}f''(\xi_j)
\]
Para algun $\xi_j \in (x_{j-1},x_j)$, con $j=1,2,\dots ,n$. Como $f''$ es continua en $[a,b]$, entonces por el Teorema de valores extremos para funciones continuas, se tiene que para $j=1,\dots , n$
\begin{align*}
    \min \limits_{x \in [a,b]} f''(x) &\leq f''(\xi_j) \leq \max \limits_{x \in [a,b]} f''(x) \\
    n \min \limits_{x \in [a,b]} f''(x) &\leq \sum_{j=1}^{n}f''(\xi_j) \leq n \max \limits_{x \in [a,b]} f''(x)\\
    \min \limits_{x \in [a,b]} f''(x) &\leq \frac{1}{n}\sum_{j=1}^{n}f''(\xi_j) \leq \max \limits_{x \in [a,b]} f''(x)
\end{align*}

Por el teorema del valor intermedio para funciones continuas, existe $\mu \in (a,b)$ tal que:
\[
f''(\mu) = \frac{1}{n}\sum_{j=1}^{n}f''(\xi_j),
\]
Y por lo tanto,
\[
\sum_{j=1}^{n}f''(\xi_j) = n f''(\mu)
\]
Usando que $h=\frac{b-a}{n}$, podemos reescribir el error independientemente de $\xi_j$ como:
\[
E(f) = -\frac{h^3}{12}\sum_{j=1}^{n}f''(\xi_j) = -\frac{h^3}{12}n f''(\mu) = -\frac{(b-a)}{12}h^2f''(\mu)
\]

\subsection{Teorema: de cuadratura gaussiana}
\subsubsection{Enunciado:}
Sea $w$ una funcion de peso positiva definida en $[a,b]$ y $q$ un polinomio no nulo de grado exactamente $n+1$ que es ortogonal a todo polinomio $p$ de grado $\leq n$, es decir $\int_a^b q(x)p(x)w(x)dx=0$. Si $x_0, x_1, \dots , x_n$ son las raices de $q$, entonces la formula\\
$\int_a^b f(x)w(x)dx \\thickapprox \sum_{i=0}^{n}a_if(x_i) \quad$ con $a_i=\int_a^bw(x)\prod_{\substack{j=0\\j\neq i}}^{n} \frac{x-x_j}{x_i-x_j}dx$\\
es exacta para todo polinomio $f$ de grado $\leq 2n+1$.

\subsection{Teorema: convergencia de métodos iterativos en sistemas lineales}
\subsubsection{Enunciado:}

Sea $b \in \mathbb{R}^n$, $A = M -N \in \mathbb{R}^{n\times n}$, donde $M$ y $A$ son matrices no singulares. Si $\parallel (M^{-1}N)\parallel<1$ para alguna norma matricial inducida entonces la sucesión generada por $x^{(k+1)}=(M^{-1}N)x^{(k)}+M^{-1}b$ para $k\geq 0$ converge a la solucion de $Ax=b$ para cualquier vector inicial $x^{(0)}$

\subsubsection{Demostración:}
Restando la ecuacion anterior de $x=(M^{-1}N)x+M^{-1}b$ donde uso la solución $x^*$, se obtiene:
\[
x^{(k+1)}-x^*=(M^{-1}N)(x^{(k)}-x^*) \] para $k\geq 0$.Ahora, utilizando alguna norma matricial inducida:
\[
\parallel x^{(k+1)}-x^*\parallel \leq \parallel M^{-1}N\parallel \parallel x^{(k)}-x^*\parallel\]

Luego, repitiendo este último paso se tiene:

\[
\parallel x^{(k+1)}-x^*\parallel \leq \parallel M^{-1}N\parallel^{k+1} \parallel x^{(0)}-x^*\parallel
\]
Usando que $\parallel M^{-1}N\parallel<1$, se concluye que:
\[
\lim_{k \to \infty} \parallel x^{(k)}-x^*\parallel = 0
\]
Para cualquier vector inicial $x^{(0)}$

\subsection{Teorema: de convergencia para matrices diagonalmente dominantes para el m´etodo de
Jacobi}
\subsubsection{Enunciado:}
Si $A$ es diagonalmente dominante, es decir, $\lvert a_{ii}\rvert \geq \sum_{j\neq i} \lvert a_{ij}\rvert$ para $i=1,\dots ,n$ y $\lvert a_{ii}\rvert > \sum_{j\neq i} \lvert a_{ij}\rvert$ para al menos un $i$, entonces la sucesión generada por el método de Jacobi converge a la solución $Ax=b$ para cualquier vector inicial $x^{(0)} \in \mathbb{R}^n$.

\subsubsection{Demostración:}
En el método de Jacobi, la matriz $M$ es la diagonal de $A$ y debe ser inversible para que el método esté bien definido , por lo que $a_{ii}\neq 0$ para $i=1,\dots ,n$. La matriz de iteracion está dada por:
\begin{align*}
    M^{-1}N = - \begin{pmatrix}
        \frac{1}{a_{11}} & 0 & \dots & 0 \\
        0 & \frac{1}{a_{22}} & \dots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \dots & \frac{1}{a_{nn}}
    \end{pmatrix}
    \begin{pmatrix}
        0 & a_{12} & \dots & a_{1n} \\
        a_{21} & 0 & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & 0
    \end{pmatrix}\\
    = - \begin{pmatrix}
        0 & \frac{a_{12}}{a_{11}} & \dots & \frac{a_{1n}}{a_{11}} \\
        \frac{a_{21}}{a_{22}} & 0 & \dots & \frac{a_{2n}}{a_{22}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{a_{n1}}{a_{nn}} & \frac{a_{n2}}{a_{nn}} & \dots & 0
    \end{pmatrix}
\end{align*}

Luego 
\[
\parallel M^{-1}N\parallel_\infty = \max_{1\leq i \leq n} \sum_{j=1}^{n} \frac{\lvert a_{ij}\rvert}{\lvert a_{ii}\rvert}<1 
\]
Pues $A$ es diagonalmente dominante. Finalmente, la convergencia es una consecuencia directa del teorema anterior.

\subsection{Teorema: Convergencia de Gauss-Seidel}
\subsubsection{Enunciado:}

Si $A$ es diagonalmente dominante, es decir, entonces la sucesion generada por el método de Gauss-Seidel converge a la solución de $Ax=b$ para cualquier vector inicial $x^{(0)} \in \mathbb{R}^n$.

\subsection{Teorema: Ínfimo de las normas matriciales inducidas}
\subsubsection{Enunciado:}
Para cada matriz $A \in \mathbb{R}^{n\times n}$, se cumple que $\rho(A) = \inf \{\parallel A\parallel \}$ sobre todas lar normas matriciales inducidas. 

\subsection{Teorema: la sucesión converge a una única solución}
\subsubsection{Enunciado:}
Una condición necesaria y suficiente para que la sucesión generada por el método iterativo $x^{(k+1)}=(M^{-1}N)x^{(k)}+M^{-1}b$  para $k\geq 0$ converja a la única solución de $Ax=b$ para todo vector inicial $x^{(0)}$ es que $\rho(M^{-1}N)<1$

\subsection{Teorema: fundamental de programación lineal}
\subsubsection{Enunciado:}
Dado un problema de PL en la forma estándar, donde $A \in \mathbb{R}^{m\times n}$, tiene rango $m$. Luego:
\begin{enumerate}
    \item si existe una solución factible, entonces existe una solución básica factible.
    \item Si existe una solución factible óptima, entonces existe una solución básica factible óptima.
\end{enumerate}  


\end{document}

